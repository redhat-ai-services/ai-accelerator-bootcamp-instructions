# Custom Notebook in RHOAI

## Need for custom notebooks

RHOAI provides many pre-built notebooks like Standard Data Science notebook, Tensorflow notebook, Pytorch notebook etc. Data scientists can spin up these notebooks and start running their experiments and creating their machine learning models without much set up.

These inbuilt notebooks come with most of the packages that one would need to run their Data Science projects. But the world of Data Science and Machine Learning is vast and there might be a need to download other python packages.

In this case, the user can use the `pip install` command inside the notebook but this might lead to issues like kernel mismatches or limited visibility and control.

The best and recommended solution in this case is to create a custom notebook. RHOAI has made it extremely easy to import a notebook image from any image registry.

In the https://www.redhat.com/en/services/training/developing-and-deploying-aiml-applications-red-hat-openshift-ai-ai267[AI267] course, the content teaches you how to create a custom notebook image by taking an existing notebook image and adding additional python packages to it. This is a great way to create a custom notebook image, but what if you or your customer needs to create a custom notebook image from scratch or need to customize the notebook image even more. Your customer may need to know what is in the notebook image itself, or they may need to add additional software (such as database drivers) to the notebook image. We can utilize the notebook repository: https://github.com/opendatahub-io/notebooks/tree/main[https://github.com/opendatahub-io/notebooks/tree/main] to create a custom notebook image from the ground up. This repository where ODH and RHOAI workbenches are being created from. Let's use this repository to create a custom notebook image.

### Notebooks Repository
The https://github.com/opendatahub-io/notebooks/tree/main[notebooks repository] uses a `makefile` to create the notebook image. The `makefile` is a file that contains a set of directives used by a make build automation tool to build a set of target images. It utilizes a  build chain feature where it builds the base image first and then builds the images on top of the base image. 

Take a look at the https://github.com/opendatahub-io/notebooks/blob/main/docs/developer-guide.md[developer guide] to understand the notebooks build chain.

The notebooks repository also has a Wiki of the different variations of images here: https://github.com/opendatahub-io/notebooks/wiki/Workbenches

## Steps to create a custom notebook

. Install _**make**_ (or _**gmake**_) on your local machine. Make is a build automation tool that automatically builds executable programs and libraries from source code by reading files called Makefiles which specify how to derive the target program. We need make _** version 4+ **_ to build the notebooks repository.

. Install _**pipenv**_ on your local machine. Pipenv is a tool that aims to bring the best of all packaging worlds (bundled dependencies, virtual environments, and package management) to the Python world. It automatically creates and manages a virtual environment for your projects, as well as adds/removes packages from your Pipfile as you install/uninstall packages.

. Start Podman or Docker on your machine.

. Log into https://quay.io[quay.io] from your web browser.

+
.If you get an error saying your account is already associated with an existing Quay account.
[%collapsible]
====
If you get this error: 

_The e-mail address your-username@redhat.com is already associated with an existing Quay account. Please log in with your username and password and associate your Red Hat account to use it in the future._

The error message you see can be remedied by the following steps:

* Please go to https://recovery.quay.io/signin[https://recovery.quay.io/signin/] and sign in with your existing Quay.io email address and password.
* Once in, open account settings, click on "External logins" on the left side, and then on the "Attach" link.
* Go to quay.io/signin and enter your RHCP data and you should be set, RHSSO button should work now. If needed try to Detach and reattach when in recovery mode.
All accounts in Quay must have unique e-mail addresses and usernames. The issue here is that there exists a user under e-mail (your email address) in Quay's db. Logging in with a user with the same e-mail address will not work because users are not connected via a special table in Quay's db and Quay will perceive logging in via that button as though a new user is coming in. Quay will then check if the e-mail address is unique and will find that it's not and will error out.
====


. Clone the bootcamp git repository with the following command:

+ 
[.console-input]
[source,terminal]
----
git clone https://github.com/opendatahub-io/notebooks.git
----

. Navigate into the folder you just cloned.

+
[.console-input]
[source,terminal]
----
cd notebooks
----

. Log into the quay.io registery from your CLI. 

+
.Generate quay.io cli password from settings
[%collapsible]
====
Log into https://quay.io[quay.io] and go to _**Account Settings**_ in the top right. In the Account settings, _**Generate Encrypted Password**_. 

[.bordershadow]
image::quay_cli_password.png[]

Enter your password and then choose the _**Podman Login**_ or _**Docker Login**_ tab. Use this when logging into quay.io from Podman or Docker.
====

+
[.console-input]
[source,terminal]
----
podman login -u='username' -p='password' quay.io
----


. Open the _**Makefile**_ and try to understand the build chain.

. Let's add a Python Package to the jupyter-data-science-ubi9-python-3.11 image. 

. Change directory to the `jupyter/datascience/ubi9-python-3.11` directory.

+
[.console-input] 
[source,terminal]
----
cd jupyter/datascience/ubi9-python-3.11 
----

. Open the _**Pipfile**_ in the `jupyter/datascience/ubi9-python-3.11` directory. This file contains the list of Python packages that are installed in the image.

. Add a new Python package to the _**Pipfile**_. For example, let's add the `art` package to the _**Pipfile**_ so we can make fancy ASCII art in our notebook.

+
After line 22, add the following line and save the file:

+
[.console-input]
[source,terminal]
----
art = "~=6.4.0"
----

. Use `pipenv lock` to add the new package to the `Pipfile.lock`. This updates the pip lock file with the new package. Run the following command:

+
[.console-input]
[source,terminal]
----
pipenv lock
----

. We now will need to update the `requirements.txt` file. The notebooks repository has a handy script that will do this for us. Run the following command (still in the `jupyter/datascience/ubi9-python-3.11` directory):

+
[.console-input]
[source,terminal]
----
../../../scripts/sync-requirements-txt.sh
----

+
You can now see that the **_Art_** package has been added to the `requirements.txt` file. As well as updating some of the existing packages.

. Now that we have added the new package to the image, we need to build the image. Change directory to the root of the repository project. After running this command you should be in the `notebooks` directory.

+
[.console-input]
[source,terminal]
----
cd ../../../
----

. Let's build the jupyter-datascience-ubi9-python-3.11 image. Run the following command:
+

[.console-input]
[source,terminal]
----
make jupyter-datascience-ubi9-python-3.11 -e  IMAGE_REGISTRY=quay.io/{quay_id}/workbench-images  -e  RELEASE=2024b
----

+
.If using gmake
[%collapsible]
====
[.console-input]
[source,terminal]
----
gmake jupyter-datascience-ubi9-python-3.11 -e  IMAGE_REGISTRY=quay.io/{quay_id}/workbench-images  -e  RELEASE=2024b
----
====

+

Note: If you're on a M-powered Mac, you need to build with `--platform linux/amd64`. In the Makefile, you can add the _**--platform linux/amd64**_ build arguments to the container build command. Edit **line 77** in the **Makefile** to be: `$(eval BUILD_ARGS := --platform linux/amd64)`

+
This takes some time to build. It will build the base image first and then build the image on top of the base image. The image will then be pushed to the quay.io registry under your account in the **_workbench_** folder.


. Check your quay registry to see the image you just built. https://quay.io/repository/{quay_id}/workbench-images?tab=tags[https://quay.io/repository/{quay_id}/workbench-images?tab=tags]

+

. A new repository named `workbench-images` will get created in your quay.io account. This will get created as a Private repository. Convert it into a public repository in the settings.

. In the `parasol-insurance` tenant (`ai-accelerator/tenants/parasol-insurance`), create a directory named `custom-workbench`

. Create the `base` and `overlays` directories inside the `custom-workbench` directory

. Create a file named `kustomization.yaml` inside the `custom-workbench/base` directory with the following content:

+
.kustomization.yaml

[.console-input]
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - custom-workbench-is.yaml
  - custom-workbench-pvc.yaml
  - custom-workbench-notebook.yaml
----

. Create a file named `custom-workbench-is.yaml` inside the `custom-workbench/base` directory with the following content:

+
.custom-workbench-is.yaml

[.console-input]
[source,yaml]
----
kind: ImageStream
apiVersion: image.openshift.io/v1
metadata:
  annotations:
    opendatahub.io/notebook-image-creator: admin
    opendatahub.io/notebook-image-desc: This is a custom notebook for running the parasol insurance code
    opendatahub.io/notebook-image-name: Custom Notebook
    opendatahub.io/notebook-image-url: 'quay.io/{quay_id}/workbench-images:jupyter-datascience-ubi9-python-3.11-2024b_{update_this}
'
    opendatahub.io/recommended-accelerators: '[]'
  name: custom-notebook
  namespace: redhat-ods-applications
  labels:
    app.kubernetes.io/created-by: byon
    opendatahub.io/dashboard: 'true'
    opendatahub.io/notebook-image: 'true'
spec:
  lookupPolicy:
    local: true
  tags:
    - name: latest
      annotations:
        opendatahub.io/notebook-python-dependencies: '[]'
        opendatahub.io/notebook-software: '[]'
        openshift.io/imported-from: 'quay.io/{quay_id}/workbench-images:jupyter-datascience-ubi9-python-3.11-2024b_{update_this}'
      from:
        kind: DockerImage
        name: 'quay.io/{quay_id}/workbench-images:jupyter-datascience-ubi9-python-3.11-2024b_{update_this}'
      importPolicy:
        importMode: Legacy
      referencePolicy:
        type: Source
----

+
[IMPORTANT]
====
Replace {quay_id} with your quay id. This ensures that the image stream you are creating references to the image you pushed to quay.io
====

. Create a file named `custom-workbench-pvc.yaml` inside the `custom-workbench/base` directory with the following content:

+
.custom-workbench-pvc.yaml

[.console-input]
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: custom-workbench
  namespace: parasol-insurance
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Gi
  volumeMode: Filesystem

----

. Create a file named `custom-workbench-notebook.yaml` inside the `custom-workbench/base` directory with the following content:

+
.custom-workbench-notebook.yaml

[.console-input]
[source,yaml]
----
apiVersion: kubeflow.org/v1
kind: Notebook
metadata:
  annotations:
    notebooks.opendatahub.io/inject-oauth: 'true'
    opendatahub.io/image-display-name: Datascience notebook
    notebooks.opendatahub.io/oauth-logout-url: ''
    opendatahub.io/accelerator-name: ''
    openshift.io/description: ''
    openshift.io/display-name: custom-workbench
    notebooks.opendatahub.io/last-image-selection: 'custom-notebook:latest'
    argocd.argoproj.io/sync-options: ServerSideApply=true
  name: custom-workbench
  namespace: parasol-insurance
spec:
  template:
    spec:
      affinity: {}
      containers:
        - name: custom-workbench
          image: 'image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/custom-notebook:latest'
          resources:
            limits:
              cpu: '2'
              memory: 8Gi
            requests:
              cpu: '1'
              memory: 8Gi
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /notebook/parasol-insurance/custom-workbench/api
              port: notebook-port
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /notebook/parasol-insurance/custom-workbench/api
              port: notebook-port
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          env:
            - name: NOTEBOOK_ARGS
              value: |-
                --ServerApp.port=8888
                --ServerApp.token=''
                --ServerApp.password=''
                --ServerApp.base_url=/notebook/parasol-insurance/custom-workbench
                --ServerApp.quit_button=False
                --ServerApp.tornado_settings={"user":"user1","hub_host":"","hub_prefix":"/projects/parasol-insurance"}
            - name: JUPYTER_IMAGE
              value: 'image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/custom-notebook:latest'
          ports:
            - containerPort: 8888
              name: notebook-port
              protocol: TCP
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /opt/app-root/src
              name: custom-workbench
            - mountPath: /dev/shm
              name: shm
          workingDir: /opt/app-root/src
      enableServiceLinks: false
      serviceAccountName: custom-workbench
      volumes:
        - name: custom-workbench
          persistentVolumeClaim:
            claimName: custom-workbench
        - emptyDir:
            medium: Memory
          name: shm
----

. Create a directory named `parasol-insurance-dev` under the `custom-workbench/overlays` directory

. Create a file named `kustomization.yaml` inside the `custom-workbench/overlays/parasol-insurance-dev` directory with the following content:

+
.kustomization.yaml

[.console-input]
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base
----

. Push the changes to the git repository

. Navigate to the `parasol-insurance` data science project in RHOAI, and notice the `custom-workbench` notebook available in the *Workbenches* tab:

+
[.bordershadow]
image::01_custom_workbench.png[Custom workbench]

[Verify]
====
Verify the new custom workbench spins up successfully and the `art` package is available in the notebook.

Use `pip list` in the notebook terminal to verify the `art` package is installed.

Create a new notebook and run the following code to see the ASCII art:

```
from art import *
hello_world=text2art("Hello World")
print(hello_world)
```

====

## Questions for Further Consideration

Additional questions that could be discussed for this topic:

. How many Python packages are included in your typical data scientist development environment? Are there any packages that are unique to your team?
. How do you handle continuous updates in your development environment, remembering that AI/ML is an evolving landscape, and new packages are released all the time, and existing packages are undergoing very frequent updates?
. Can data scientists ask for new packages in a securely controlled development environment?
. Where do you store source code for model experimentation and training?
. Do you think that cluster storage (such as an OpenShift PVC) is a good permanent location for source code, so that in the event of failure the source is not lost?
. How do your teams of data scientists collaborate on notebooks when training models or performing other experiments?
